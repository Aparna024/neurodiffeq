{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does It Work?\n",
    "\n",
    "To solve a differential equation. We need the solution to satisfy 2 things:\n",
    "1. They need to satisfy the equation\n",
    "2. They need to satisfy the initial/boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisfying the Equation\n",
    "\n",
    "The key idea of solving differential equations with ANNs is to reformulate the problem as an optimization problem in which we minimize the residual of the differential equations.  In a very general sense, a differential equation can be expressed as\n",
    "$$\\mathcal{L}u - f = 0$$\n",
    "where $\\mathcal{L}$ is the differential operator, $u\\left(x,t\\right)$ is the solution that we wish to find, and $f$ is a known forcing function.  Note that $\\mathcal{L}$ contains any combination of temporal and spatial partial derivatives.  Moreover, $x$ is in general a vector in three spatial dimensions and $u$ is a vector of solutions.  We denote the output of the neural network as\n",
    "$u_{N}\\left(x, t; p\\right)$ where the parameter vector $p$ is a vector containing the weights and biases of the neural network.  We will drop the arguments of the neural network solution in order to be concise.  If $u_{N}$ is a solution to the differential equation, then the residual $$\\mathcal{R}\\left(u_{N}\\right) = \\mathcal{L}u_{N} - f $$ \n",
    "will be identically zero.  One way to incorporate this into the training process of a neural network is to use the residual as the loss function.  In general, the $L^{2}$ loss of the residual is used.  This is the convention that ``NeuroDiffEq`` follows,  although we note that other loss functions could be conceived.  Solving the differential equation is re-cast as the following optimization problem: \n",
    "$$\n",
    "\\min_{p}\\left(\\mathcal{L}u_{N} - f\\right)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satisfying the Initial/Boundary Conditions\n",
    "\n",
    "It is necessary to inform the neural network about any boundary and initial conditions since it has no way of enforcing these *a priori*. There are two primary ways to satisfy the boundary and initial conditions.  First, one can impose the initial/boundary\n",
    "conditions in the loss function.  For example, given an initial condition $u\\left(x,t_{0}\\right) = u_{0}\\left(x\\right)$, the loss function can be modified to:\n",
    "$$\n",
    "\\min_{p}\\left[\\left(\\mathcal{L}u_{N} - f\\right)^2 + \\lambda\\left(u_{N}\\left(x,t_{0}\\right) - u_0\\left(x\\right)\\right)^2\\right]\n",
    "$$\n",
    "where the second term penalizes solutions that don't satisfy the initial condition.  Larger values of the regularization parameter $\\lambda$ result in stricter satisfaction of the initial condition while sacrificing solution accuracy.  However, this approach does not lead to *exact* satisfaction of the initial and boundary conditions.\n",
    "\n",
    "Another option is to transform the $u_{N}$ in a way such that the initial/boundary conditions are satisfied by\n",
    "construction.  Given an initial condition $u_{0}\\left(x\\right)$ the neural network can be transformed according to:\n",
    "$$\n",
    "\\widetilde{u}\\left(x,t\\right) = u_{0}\\left(x\\right) + \\left(1-e^{-\\left(t-t_{0}\\right)}\\right)u_{N}\\left(x,t\\right)\n",
    "$$\n",
    "so that when $t = t_0$, $\\widetilde{u}$ will always be $u_0$. Accordingly, the objective function becomes \n",
    "$$\n",
    "\\min_{p}\\left(\\mathcal{L}\\widetilde{u} - f\\right)^2.\n",
    "$$\n",
    "This approach is similar to the trial function approach, but with a different form of the trial\n",
    "function.  Modifying the neural network to account for boundary conditions can also be done.  In general, the transformed solution will have the form:\n",
    "$$\n",
    "\\widetilde{u}\\left(x,t\\right) = A\\left(x, t; x_{\\text{boundary}}, t_{0}\\right)u_{N}\\left(x,t\\right)\n",
    "$$\n",
    "where $A\\left(x, t; x_{\\text{boundary}}, t_{0}\\right)$  must be designed so that $\\widetilde{u}\\left(x,t\\right)$ has the correct boundary conditions.  This can be very challenging for complicated domains.\n",
    "\n",
    "Both of these two methods have their advantages. The first way is simpler to implement and can be more easily extended to high-dimensional PDEs and PDEs formulated on complicated domains. The second way assures that the initial/boundary conditions are exactly satisfied.  Considering that differential equations can be sensitive to initial/boundary conditions, this is expected to play an important role. Another advantage of the second method is that fixing these conditions can reduce the effort required during the training of the ANN. ``NeuroDiffEq`` employs the second approach. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
